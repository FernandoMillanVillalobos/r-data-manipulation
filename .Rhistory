data_frames <- map_dfr(csv_files_dir_ls, ~ read_csv(.x, show_col_types = FALSE))
glimpse(data_frames)
# and with a new column representing the file name
map_dfr(csv_files_dir_ls, ~ read_csv(.x, , show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
# using directly read_csv
read_csv(csv_files_dir_ls, id = "filename", show_col_types = FALSE) %>%
glimpse
# inconsistent column names
# generating the samples with inconsistent column names
mpg_samples2 <- map(1:10, ~ slice_sample(mpg, n = 20))
inconsistent_dframes <- map(mpg_samples2, ~ janitor::clean_names(dat = .x, case = "random"))
map(inconsistent_dframes, ~ colnames(.x)) %>%
head
# selecting a random set of columns per data frame
inconsistent_dframes <- map(inconsistent_dframes, ~ .x[sample(1:length(.x), sample(1:length(.x), 1))])
map(inconsistent_dframes, ~ colnames(.x)) %>%
head()
# saving to disk
dir_create(c("input/unclean_files"))
iwalk(inconsistent_dframes, ~ write_csv(.x, paste0("input/unclean_files/", .y, ".csv")))
# loading and cleaning the data frames
many_columns_data_frame <- dir_ls(path = "input/unclean_files/", glob = "*.csv", type = "file") %>%
map_dfr(~ read_csv(.x, name_repair = tolower, show_col_types = FALSE) %>%
mutate(filename = .x))
# showing results
many_columns_data_frame %>%
glimpse()
# files not in the same folder
mpg_samples <- map(1:40, ~ slice_sample(mpg, n = 20))
# Create directories
dir_create(c("input/nested_folders", "nested_folders/first_nested_folder", "nested_folders/second_nested_folder"))
# First folder
iwalk(mpg_samples[1:20], ~ write_csv(.x, paste0("nested_folders/first_nested_folder/", .y, "_first.csv")))
# Second folder
iwalk(mpg_samples[21:40], ~ write_csv(.x, paste0("nested_folders/second_nested_folder/", .y, "_second.csv")))
# searching through nested folders recursively
(csv_files_nested <- dir_ls("input/nested_folders/", glob = "*.csv", type = "file", recurse = TRUE))
mpg_samples <- map(1:40, ~ slice_sample(mpg, n = 20))
# Create directories
dir_create(c("input/nested_folders", "nested_folders/first_nested_folder", "nested_folders/second_nested_folder"))
# First folder
iwalk(mpg_samples[1:20], ~ write_csv(.x, paste0("nested_folders/first_nested_folder/", .y, "_first.csv")))
# Second folder
iwalk(mpg_samples[21:40], ~ write_csv(.x, paste0("nested_folders/second_nested_folder/", .y, "_second.csv")))
# searching through nested folders recursively
(csv_files_nested <- dir_ls("input/nested_folders/", glob = "*.csv", type = "file", recurse = TRUE))
styler:::style_selection()
(csv_files_nested <- dir_ls("input/nested_folders/", glob = "*.csv", type = "file", recurse = TRUE))
map_dfr(csv_files_nested, ~ read_csv(.x, show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
mpg_samples <- map(1:40, ~ slice_sample(mpg, n = 20))
# Create directories
dir_create(c("input/nested_folders", "input/nested_folders/first_nested_folder", "input/nested_folders/second_nested_folder"))
# First folder
iwalk(mpg_samples[1:20], ~ write_csv(.x, paste0("nested_folders/first_nested_folder/", .y, "_first.csv")))
# Second folder
iwalk(mpg_samples[21:40], ~ write_csv(.x, paste0("nested_folders/second_nested_folder/", .y, "_second.csv")))
# searching through nested folders recursively
(csv_files_nested <- dir_ls("input/nested_folders/", glob = "*.csv", type = "file", recurse = TRUE))
map_dfr(csv_files_nested, ~ read_csv(.x, show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
many_columns_data_frame %>%
glimpse()
# files not in the same folder
mpg_samples3 <- map(1:40, ~ slice_sample(mpg, n = 20))
# Create directories
dir_create(c("input/nested_folders", "input/nested_folders/first_nested_folder", "input/nested_folders/second_nested_folder"))
# First folder
iwalk(mpg_samples[1:20], ~ write_csv(.x, paste0("nested_folders/first_nested_folder/", .y, "_first.csv")))
# Second folder
iwalk(mpg_samples[21:40], ~ write_csv(.x, paste0("nested_folders/second_nested_folder/", .y, "_second.csv")))
# searching through nested folders recursively
(csv_files_nested <- dir_ls("input/nested_folders/", glob = "*.csv", type = "file", recurse = TRUE))
map_dfr(csv_files_nested, ~ read_csv(.x, show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
View(mpg_samples3)
View(mpg_samples3)
iwalk(mpg_samples[1:20], ~ write_csv(.x, paste0("input/nested_folders/first_nested_folder/", .y, "_first.csv")))
# Second folder
iwalk(mpg_samples[21:40], ~ write_csv(.x, paste0("input/nested_folders/second_nested_folder/", .y, "_second.csv")))
# searching through nested folders recursively
(csv_files_nested <- dir_ls("input/nested_folders/", glob = "*.csv", type = "file", recurse = TRUE))
map_dfr(csv_files_nested, ~ read_csv(.x, show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
csv_files_nested[str_detect(csv_files_nested, pattern = "[2-4]_first|second\\.csv$", negate = TRUE)] %>%
map_dfr(~ read_csv(.x, show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
## By default, show code for all chunks in the knitted document,
## as well as the output. To override for a particular chunk
## use echo = FALSE in its options.
knitr::opts_chunk$set(
echo=TRUE, message=FALSE, warning=FALSE
)
# CONFIG
user_name <- "fernandomillanvillalobos" # your Git username (only needed if
# you want to deploy to GH pages)
project_name <- "rddj-template" # adapt!
package_date <- "2022-05-01" # date of the CRAN snapshot that
# the checkpoint package uses
r_version <- "4.2.0" # R-Version to use
options(Ncpus = 4) # use 4 cores for parallelized installation of packages
if (r_version != paste0(version$major, ".", version$minor)) {
stop("ERROR: specified R version does not match currently used.")
}
detach_all_packages <- function() {
basic_packages_blank <-  c("stats",
"graphics",
"grDevices",
"utils",
"datasets",
"methods",
"base")
basic_packages <- paste("package:", basic_packages_blank, sep = "")
package_list <- search()[
ifelse(unlist(gregexpr("package:", search())) == 1, TRUE, FALSE)]
package_list <- setdiff(package_list, basic_packages)
if (length(package_list) > 0)  for (package in package_list) {
detach(package, character.only = TRUE, unload = TRUE)
print(paste("package ", package, " detached", sep = ""))
}
}
detach_all_packages()
# this allows multiple persons to use the same RMarkdown
# without adjusting the working directory by themselves all the time
source("scripts/csf.R")
path_to_wd <- csf() # if this - for some reason - does not work,
# replace with a hardcoded path, like so: "~/projects/rddj-template/analysis/"
if (is.null(path_to_wd) | !dir.exists(path_to_wd)) {
print("WARNING: No working directory specified for current user")
} else {
setwd(path_to_wd)
}
# suppress scientific notation
options(scipen = 999)
# suppress summarise info
options(dplyr.summarise.inform = FALSE)
# unload global rstudioapi and knitr again to avoid conflicts with checkpoint
# this is only necessary if executed within RStudio
# outside of RStudio, namely in the knit.sh script, this causes RMarkdown
# rendering to fail, thus should not be executed there
if (Sys.getenv("RSTUDIO") == "1") {
detach_all_packages()
}
# from https://mran.revolutionanalytics.com/web/packages/\
# checkpoint/vignettes/using-checkpoint-with-knitr.html
# if you don't need a package, remove it from here (commenting not sufficient)
# tidyverse: see https://blog.rstudio.org/2016/09/15/tidyverse-1-0-0/
cat("
library(rstudioapi)
library(tidyverse)
library(tidylog)
library(jsonlite)
library(lintr)
library(rmarkdown)
library(rio)
library(cowplot)
library(extrafont)
library(ggrepel)
library(scales)
library(pacman)
library(htmltab)
library(rmiscutils)
library(RSQLite)
library(fs)
library(openxlsx)
library(waldo)
library(vcdExtra)
library(psych)
library(Hmisc)
library(skimr)
library(janitor)",
file = "manifest.R")
# if checkpoint is not yet installed, install it (for people using this
# system for the first time)
if (!require(checkpoint)) {
if (!require(devtools)) {
install.packages("devtools", repos = "http://cran.us.r-project.org")
require(devtools)
}
devtools::install_github("RevolutionAnalytics/checkpoint",
ref = "v0.3.2", # could be adapted later,
# as of now (beginning of July 2017
# this is the current release on CRAN)
repos = "http://cran.us.r-project.org")
require(checkpoint)
}
# nolint start
if (!dir.exists("~/.checkpoint")) {
dir.create("~/.checkpoint")
}
# nolint end
# install packages for the specified CRAN snapshot date
checkpoint(snapshot_date = package_date,
project = path_to_wd,
verbose = T,
scanForPackages = T,
use.knitr = F,
R.version = r_version)
rm(package_date)
source("manifest.R")
unlink("manifest.R")
sessionInfo()
# if you want to outsource logic to other script files, see README for
# further information
# Load all visualizations functions as separate scripts
knitr::read_chunk("scripts/dviz.supp.R")
source("scripts/dviz.supp.R")
knitr::read_chunk("scripts/themes.R")
source("scripts/themes.R")
knitr::read_chunk("scripts/plot_grid.R")
source("scripts/plot_grid.R")
knitr::read_chunk("scripts/align_legend.R")
source("scripts/align_legend.R")
knitr::read_chunk("scripts/label_log10.R")
source("scripts/label_log10.R")
knitr::read_chunk("scripts/outliers.R")
source("scripts/outliers.R")
salary <- c(18700000, 14626720, 14137500, 13980000, 12916666)
position <- c("QB", "QB", "DE", "QB", "QB")
team <- c("Colts", "Patriots", "Panthers", "Bengals", "Giants")
name.last <- c("Manning", "Brady", "Pepper", "Palmer", "Manning")
name.first <- c("Peyton", "Tom", "Julius", "Carson", "Eli")
top.5.salaries <- data.frame(name.last, name.first, team, position, salary)
top.5.salaries
# calling the built-in data editor
# top.5.salaries <- edit(top.5.salaries)
# fix(top.5.salaries)
# using scan
# names <- scan(what = "")
# names
# names2 = scan(what=list(a=0,b="",c=0))
# names2
# creating a matrix
# mymat <- matrix(scan(), ncol = 3, byrow = TRUE)
# mymat
snowdata <- read.table("input/BostonWinterSnowfalls.csv", header = TRUE, sep = ",", quote = "\"")
# getting data online
sp500 <- read.csv("http://bit.ly/BostonSnowfallCSV", sep="")
# getting data with no delimiters
ff <- tempfile()
cat(file = ff, "New York, NY 66,834.6
Kings, NY 34,722.9
Bronx, NY 31,729.8
Queens, NY 20,453.0
San Francisco, CA 16,526.2
Hudson, NJ 12,956.9
Suffolk, MA 11,691.6
Philadelphia, PA 11,241.1
Washington, DC 9,378.0
Alexandria IC, VA 8,552.2")
city <- read.fwf(ff, widths = c(18, -19, 8), as.is = TRUE)
city
# write.table(snowdata, file = "output/snowdata.txt", quote = FALSE, sep = ",", row.names = FALSE)
# write.csv(snowdata, file = "output/snowdata.csv", row.names = FALSE)
# to connect with an external database
# drv <- dbDriver("SQLite")
# con <- dbConnect(drv, dbname = system.file("extdata", "bb.db", package = "nutshell"))
# creating our database
mydb <- dbConnect(RSQLite::SQLite(), "")
dbWriteTable(mydb, "mtcars", mtcars)
dbWriteTable(mydb, "iris", iris)
dbListTables(mydb)
# Issue a query with dbGetQuery()
dbGetQuery(mydb, 'SELECT * FROM mtcars LIMIT 5')
# disconnecting from dabase
dbDisconnect(mydb)
# janitor approach
mpg_new <- read_csv("input/mpg_uppercase.csv") %>%
janitor::clean_names() %>%
select(c(manufacturer, model)) %>%
glimpse()
# tidyverse approach
read_csv("input/mpg_uppercase.csv", name_repair = make_clean_names) %>%
glimpse()
# replacing and removing character strings with make_clean_names
make_clean_names(c("A", "B%", "C"), replace = c("%" = "_percent"))
# with reg expressions
make_clean_names(c("A_1", "B_1", "C_1"), replace = c("^A_" = "a"))
# snake naming convention per default
make_clean_names(c("myHouse", "MyGarden"), case = "snake")
make_clean_names(c("myHouse", "MyGarden"), case = "none")
read_csv("input/mpg_uppercase.csv", name_repair = ~ make_clean_names(., case = "upper_camel")) %>% # The dot . in make_clean_names denotes the vector of column names.
glimpse()
# selecting specific columns
read_csv("input/mpg_uppercase.csv", show_col_types = FALSE, name_repair = make_clean_names, col_select = c(manufacturer, model)) %>%
glimpse()
horas_sol <- read_csv("input/SS_STAID001395.txt", skip = 19) |> # los datos empiezan en la linea 20
janitor::clean_names()
head(horas_sol)
# .xlsx files
# importing .xls file
emisiones <- readxl::read_xls("input/env_air_gge.xls", sheet = 1, skip = 362, n_max = 36)
head(emisiones)
# iterate over multiple worksheets in a workbook
path <- "input/madrid_temp.xlsx"
mad <- path %>%
readxl::excel_sheets() %>%
set_names() %>%
map_df(readxl::read_excel,
path = path, .id = "yr"
)
head(mad)
# importing and reading several .xlsx files at once
# without merging
dir_ls("input", regexp = "xlsx") %>%
map(readxl::read_excel)
# merging into a new column
data_df <- dir_ls("input", regexp = "xlsx") %>%
map_df(readxl::read_excel, .id = "city")
# cleaning city column
data_df <- mutate(data_df, city = path_file(city) %>%
path_ext_remove() %>%
str_replace("_temp", ""))
head(data_df)
# .csv files
# adding new directory
dir_create("input", c("many_files"))
# creating random samples from mpg data set
mpg_samples <- map(1:25, ~ slice_sample(mpg, n = 20))
# adding .csv files from samples to the new directory
iwalk(mpg_samples, ~ write_csv(., paste0("input/many_files/", .y, ".csv")))
# creating a character vector of file paths
# with list.files from Base-R
(csv_files_list_files <- list.files(path = "input/many_files", pattern = "csv", full.names = TRUE))
# with dir_ls from fs package
(csv_files_dir_ls <- dir_ls(path = "input/many_files/", glob = "*.csv", type = "file"))
# reading the files from a character vector of paths
data_frames <- map_dfr(csv_files_dir_ls, ~ read_csv(.x, show_col_types = FALSE))
glimpse(data_frames)
# and with a new column representing the file name
map_dfr(csv_files_dir_ls, ~ read_csv(.x, , show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
# using directly read_csv
read_csv(csv_files_dir_ls, id = "filename", show_col_types = FALSE) %>%
glimpse
# inconsistent column names
# generating the samples with inconsistent column names
mpg_samples2 <- map(1:10, ~ slice_sample(mpg, n = 20))
inconsistent_dframes <- map(mpg_samples2, ~ janitor::clean_names(dat = .x, case = "random"))
map(inconsistent_dframes, ~ colnames(.x)) %>%
head
# selecting a random set of columns per data frame
inconsistent_dframes <- map(inconsistent_dframes, ~ .x[sample(1:length(.x), sample(1:length(.x), 1))])
map(inconsistent_dframes, ~ colnames(.x)) %>%
head()
# saving to disk
dir_create(c("input/unclean_files"))
iwalk(inconsistent_dframes, ~ write_csv(.x, paste0("input/unclean_files/", .y, ".csv")))
# loading and cleaning the data frames
many_columns_data_frame <- dir_ls(path = "input/unclean_files/", glob = "*.csv", type = "file") %>%
map_dfr(~ read_csv(.x, name_repair = tolower, show_col_types = FALSE) %>%
mutate(filename = .x))
# showing results
many_columns_data_frame %>%
glimpse()
# files not in the same folder
mpg_samples3 <- map(1:40, ~ slice_sample(mpg, n = 20))
# Create directories
dir_create(c("input/nested_folders", "input/nested_folders/first_nested_folder", "input/nested_folders/second_nested_folder"))
# First folder
iwalk(mpg_samples[1:20], ~ write_csv(.x, paste0("input/nested_folders/first_nested_folder/", .y, "_first.csv")))
# Second folder
iwalk(mpg_samples[21:40], ~ write_csv(.x, paste0("input/nested_folders/second_nested_folder/", .y, "_second.csv")))
# janitor approach
mpg_new <- read_csv("input/mpg_uppercase.csv", show_col_types = FALSE) %>%
janitor::clean_names() %>%
select(c(manufacturer, model)) %>%
glimpse()
# tidyverse approach
read_csv("input/mpg_uppercase.csv", name_repair = make_clean_names) %>%
glimpse()
# replacing and removing character strings with make_clean_names
make_clean_names(c("A", "B%", "C"), replace = c("%" = "_percent"))
# with reg expressions
make_clean_names(c("A_1", "B_1", "C_1"), replace = c("^A_" = "a"))
# snake naming convention per default
make_clean_names(c("myHouse", "MyGarden"), case = "snake")
make_clean_names(c("myHouse", "MyGarden"), case = "none")
read_csv("input/mpg_uppercase.csv", name_repair = ~ make_clean_names(., case = "upper_camel")) %>% # The dot . in make_clean_names denotes the vector of column names.
glimpse()
# selecting specific columns
read_csv("input/mpg_uppercase.csv", show_col_types = FALSE, name_repair = make_clean_names, col_select = c(manufacturer, model)) %>%
glimpse()
# janitor approach
mpg_new <- read_csv("input/mpg_uppercase.csv", show_col_types = FALSE) %>%
janitor::clean_names() %>%
select(c(manufacturer, model)) %>%
glimpse()
# tidyverse approach
read_csv("input/mpg_uppercase.csv", name_repair = make_clean_names, show_col_types = FALSE) %>%
glimpse()
# replacing and removing character strings with make_clean_names
make_clean_names(c("A", "B%", "C"), replace = c("%" = "_percent"))
# with reg expressions
make_clean_names(c("A_1", "B_1", "C_1"), replace = c("^A_" = "a"))
# snake naming convention per default
make_clean_names(c("myHouse", "MyGarden"), case = "snake")
make_clean_names(c("myHouse", "MyGarden"), case = "none")
read_csv("input/mpg_uppercase.csv", show_col_types = FALSE, name_repair = ~ make_clean_names(., case = "upper_camel")) %>% # The dot . in make_clean_names denotes the vector of column names.
glimpse()
# selecting specific columns
read_csv("input/mpg_uppercase.csv", show_col_types = FALSE, name_repair = make_clean_names, col_select = c(manufacturer, model)) %>%
glimpse()
(csv_files_list_files <- list.files(path = "input/many_files", pattern = "csv", full.names = TRUE))
# with dir_ls from fs package
(csv_files_dir_ls <- dir_ls(path = "input/many_files/", glob = "*.csv", type = "file"))
# reading the files from a character vector of paths
data_frames <- map_dfr(csv_files_dir_ls, ~ read_csv(.x, show_col_types = FALSE))
glimpse(data_frames)
# and with a new column representing the file name
map_dfr(csv_files_dir_ls, ~ read_csv(.x, , show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
# using directly read_csv
read_csv(csv_files_dir_ls, id = "filename", show_col_types = FALSE) %>%
glimpse
horas_sol <- read_csv("input/SS_STAID001395.txt", skip = 19) |> # los datos empiezan en la linea 20
janitor::clean_names()
head(horas_sol)
# .xlsx files
# importing .xls file
emisiones <- readxl::read_xls("input/env_air_gge.xls", sheet = 1, skip = 362, n_max = 36)
head(emisiones)
# iterate over multiple worksheets in a workbook
path <- "input/madrid_temp.xlsx"
mad <- path %>%
readxl::excel_sheets() %>%
set_names() %>%
map_df(readxl::read_excel,
path = path, .id = "yr"
)
head(mad)
# importing and reading several .xlsx files at once
# without merging
dir_ls("input", regexp = "xlsx") %>%
map(readxl::read_excel)
# merging into a new column
data_df <- dir_ls("input", regexp = "xlsx") %>%
map_df(readxl::read_excel, .id = "city")
# cleaning city column
data_df <- mutate(data_df, city = path_file(city) %>%
path_ext_remove() %>%
str_replace("_temp", ""))
head(data_df)
# .csv files
# adding new directory
# dir_create("input", c("many_files"))
# creating random samples from mpg data set
# mpg_samples <- map(1:25, ~ slice_sample(mpg, n = 20))
# adding .csv files from samples to the new directory
# iwalk(mpg_samples, ~ write_csv(., paste0("input/many_files/", .y, ".csv")))
# creating a character vector of file paths
# with list.files from Base-R
(csv_files_list_files <- list.files(path = "input/many_files", pattern = "csv", full.names = TRUE))
# with dir_ls from fs package
(csv_files_dir_ls <- dir_ls(path = "input/many_files/", glob = "*.csv", type = "file"))
# reading the files from a character vector of paths
data_frames <- map_dfr(csv_files_dir_ls, ~ read_csv(.x, show_col_types = FALSE))
glimpse(data_frames)
# and with a new column representing the file name
map_dfr(csv_files_dir_ls, ~ read_csv(.x, , show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
# using directly read_csv
read_csv(csv_files_dir_ls, id = "filename", show_col_types = FALSE) %>%
glimpse
# inconsistent column names
# generating the samples with inconsistent column names
mpg_samples2 <- map(1:10, ~ slice_sample(mpg, n = 20))
inconsistent_dframes <- map(mpg_samples2, ~ janitor::clean_names(dat = .x, case = "random"))
map(inconsistent_dframes, ~ colnames(.x)) %>%
head
# selecting a random set of columns per data frame
inconsistent_dframes <- map(inconsistent_dframes, ~ .x[sample(1:length(.x), sample(1:length(.x), 1))])
map(inconsistent_dframes, ~ colnames(.x)) %>%
head()
# saving to disk
# dir_create(c("input/unclean_files"))
# iwalk(inconsistent_dframes, ~ write_csv(.x, paste0("input/unclean_files/", .y, ".csv")))
# loading and cleaning the data frames
many_columns_data_frame <- dir_ls(path = "input/unclean_files/", glob = "*.csv", type = "file") %>%
map_dfr(~ read_csv(.x, name_repair = tolower, show_col_types = FALSE) %>%
mutate(filename = .x))
# showing results
many_columns_data_frame %>%
glimpse()
# files not in the same folder
mpg_samples3 <- map(1:40, ~ slice_sample(mpg, n = 20))
# Create directories
# dir_create(c("input/nested_folders", "input/nested_folders/first_nested_folder", "input/nested_folders/second_nested_folder"))
# First folder
# iwalk(mpg_samples[1:20], ~ write_csv(.x, paste0("input/nested_folders/first_nested_folder/", .y, "_first.csv")))
# Second folder
# iwalk(mpg_samples[21:40], ~ write_csv(.x, paste0("input/nested_folders/second_nested_folder/", .y, "_second.csv")))
# searching through nested folders recursively
(csv_files_nested <- dir_ls("input/nested_folders/", glob = "*.csv", type = "file", recurse = TRUE))
map_dfr(csv_files_nested, ~ read_csv(.x, show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
# selecting the files to import from a string pattern
csv_files_nested[str_detect(csv_files_nested, pattern = "[2-4]_first|second\\.csv$", negate = TRUE)] %>%
map_dfr(~ read_csv(.x, show_col_types = FALSE) %>%
mutate(filename = .x)) %>%
glimpse()
